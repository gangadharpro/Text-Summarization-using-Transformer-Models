# -*- coding: utf-8 -*-
"""streamlit_ap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ThK8LooqQO6wqPNKsv_oVFGNITOLjrDy
"""

import streamlit as st
import re
import string
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dot, Activation, Concatenate, TimeDistributed, Dropout, Bidirectional
from transformers import BartTokenizer, BartForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration
from rouge_score import rouge_scorer

# --- Basic Preprocessing Function ---
def clean_text(text):
    text = text.lower()
    text = re.sub(r"\[(.*?)\]", '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", '', text)
    return text.strip()

# --- LSTM Model Setup ---
vocab_size = 3000
max_article_len = 400
max_summary_len = 40
embedding_dim = 100
latent_dim = 256

# Dummy tokenizers for this demo (replace with trained ones)
article_tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
article_tokenizer.fit_on_texts(["dummy init"])
summary_tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>", filters='')
summary_tokenizer.fit_on_texts(["<start> dummy summary <end>"])
reverse_target_word_index = summary_tokenizer.index_word
target_word_index = summary_tokenizer.word_index

embedding_layer = Embedding(vocab_size, embedding_dim, trainable=False)

encoder_inputs = Input(shape=(max_article_len,))
enc_emb = embedding_layer(encoder_inputs)
enc_emb = Dropout(0.2)(enc_emb)
encoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3))
encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_emb)
state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

# Decoder Setup
decoder_inputs = Input(shape=(max_summary_len - 1,))
dec_emb = embedding_layer(decoder_inputs)
dec_emb = Dropout(0.2)(dec_emb)
decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True, dropout=0.3)
dec_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])

score = Dot(axes=[2, 2])([dec_outputs, encoder_outputs])
attention_weights = Activation('softmax')(score)
context_vector = Dot(axes=[2, 1])([attention_weights, encoder_outputs])
decoder_combined_context = Concatenate(axis=-1)([context_vector, dec_outputs])
output = TimeDistributed(Dense(vocab_size, activation="softmax"))(decoder_combined_context)

model = Model([encoder_inputs, decoder_inputs], output)

# Inference setup
encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])

decoder_state_input_h = Input(shape=(latent_dim * 2,))
decoder_state_input_c = Input(shape=(latent_dim * 2,))
decoder_hidden_state_input = Input(shape=(max_article_len, latent_dim * 2))
decoder_input_single = Input(shape=(1,))

dec_emb2 = embedding_layer(decoder_input_single)
dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])
score2 = Dot(axes=[2, 2])([dec_outputs2, decoder_hidden_state_input])
attention_weights2 = Activation('softmax')(score2)
context_vector2 = Dot(axes=[2, 1])([attention_weights2, decoder_hidden_state_input])
decoder_combined_context2 = Concatenate(axis=-1)([context_vector2, dec_outputs2])
output_tokens = TimeDistributed(Dense(vocab_size, activation="softmax"))(decoder_combined_context2)

decoder_model = Model([
    decoder_input_single, decoder_hidden_state_input,
    decoder_state_input_h, decoder_state_input_c
], [output_tokens, state_h2, state_c2])

# Decode Logic
def decode_sequence(input_seq):
    enc_out, enc_h, enc_c = encoder_model.predict(input_seq, verbose=0)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_word_index.get('<start>', 1)
    decoded_sentence = []

    for _ in range(max_summary_len):
        output_tokens, h, c = decoder_model.predict(
            [target_seq, enc_out, enc_h, enc_c], verbose=0)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_token = reverse_target_word_index.get(sampled_token_index, '')
        if sampled_token in ('<end>', '<OOV>', ''):
            break
        decoded_sentence.append(sampled_token)
        target_seq[0, 0] = sampled_token_index
        enc_h, enc_c = h, c

    return ' '.join(decoded_sentence)

# --- Load Transformer Models ---
bart_tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
bart_model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
t5_tokenizer = T5Tokenizer.from_pretrained("t5-small")
t5_model = T5ForConditionalGeneration.from_pretrained("t5-small")

# --- CNN Dummy Extractive ---
def extractive_summary(text, num_sentences=3):
    sentences = text.split('.')
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    return '. '.join(sentences[:num_sentences])

# --- Streamlit UI ---
st.title("üìö Text Summarization App")
user_input = st.text_area("üìù Enter article text for summarization:")

if st.button("Generate Summaries") and user_input:
    cleaned = clean_text(user_input)
    st.markdown("### üìë Summaries")

    # CNN
    cnn_summary = extractive_summary(cleaned)
    st.subheader("üåÄ CNN Extractive Summary")
    st.write(cnn_summary)

    # LSTM
    lstm_input = pad_sequences(article_tokenizer.texts_to_sequences([cleaned]), maxlen=max_article_len, padding='post')
    lstm_summary = decode_sequence(lstm_input)
    st.subheader("üß† LSTM Abstractive Summary")
    st.write(lstm_summary)

    # BART
    bart_inputs = bart_tokenizer([user_input], max_length=1024, truncation=True, return_tensors='pt')
    bart_ids = bart_model.generate(bart_inputs['input_ids'], max_length=130, min_length=30)
    bart_summary = bart_tokenizer.decode(bart_ids[0], skip_special_tokens=True)
    st.subheader("üìò BART Summary")
    st.write(bart_summary)

    # T5
    t5_input = "summarize: " + user_input
    t5_tokens = t5_tokenizer.encode(t5_input, return_tensors='pt', max_length=512, truncation=True)
    t5_ids = t5_model.generate(t5_tokens, max_length=150, min_length=40)
    t5_summary = t5_tokenizer.decode(t5_ids[0], skip_special_tokens=True)
    st.subheader("üìó T5 Summary")
    st.write(t5_summary)

    # ROUGE Scores
    st.markdown("---")
    st.subheader("üìä ROUGE Score Comparison")
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    reference = user_input

    for model_name, gen_sum in zip(['CNN', 'LSTM', 'BART', 'T5'], [cnn_summary, lstm_summary, bart_summary, t5_summary]):
        scores = scorer.score(reference, gen_sum)
        st.markdown(f"**{model_name}**")
        st.text(f"ROUGE-1: {scores['rouge1'].fmeasure:.4f} | ROUGE-2: {scores['rouge2'].fmeasure:.4f} | ROUGE-L: {scores['rougeL'].fmeasure:.4f}")

