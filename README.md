# Text-Summarization-using-Transformer-Models
This project compares Transformer-based, CNN-based extractive, and LSTM-based abstractive models for long-form text summarization. It evaluates accuracy, readability, and coherence using the CNN/Daily Mail dataset, aiming to determine if Transformers outperform traditional models. 

# ğŸ§  Text Summarization Using Transformer Models

This project explores and compares multiple text summarization techniques â€” from traditional extractive methods to advanced transformer-based models â€” using the **CNN/DailyMail dataset**. It also includes a **Streamlit web app** to interact with summarization models in real-time.

---

## ğŸ“Œ Project Overview

- ğŸ” **Goal**: Evaluate which model best balances accuracy and efficiency for long-form text summarization.
- ğŸ§ª **Models Implemented**:
  - CNN-based Extractive Summarization (TF-IDF + KMeans)
  - LSTM Abstractive Summarization (Base + Enhanced with Attention & GloVe)
  - Transformer-Based Abstractive Summarizers (BART & T5)
- ğŸ“Š **Evaluation Metric**: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)
- ğŸŒ **Live Demo**: Streamlit app for input article â†’ model-wise summary comparison

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ data_loader.py               # Dataset reading and subsampling
â”œâ”€â”€ preprocessing.py            # Text cleaning, tokenization, EDA
â”œâ”€â”€ cnn_extractive.py           # TF-IDF + KMeans based summarizer
â”œâ”€â”€ lstm_base.py                # Basic LSTM seq2seq summarizer
â”œâ”€â”€ lstm_enhanced.py           # LSTM with attention and GloVe
â”œâ”€â”€ transformer_models.py       # BART and T5 summarizers using Hugging Face
â”œâ”€â”€ evaluation.py               # ROUGE metrics + visualizations
â”œâ”€â”€ app.py                      # Streamlit app
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ organized_project_notebook.ipynb  # All code in one structured notebook
