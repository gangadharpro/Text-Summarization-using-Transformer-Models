# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sSyqaZ4KK-paEynhZybbudwVmvQewxq-
"""

# data_loader.py
import os
import pandas as pd
from datasets import load_dataset

def load_cnn_dailymail_dataset(subset_percent=30):
    print(f"Loading {subset_percent}% of CNN/Daily Mail dataset...")
    subset_str = f"train[:{subset_percent}%]"
    val_str = f"validation[:{subset_percent}%]"
    test_str = f"test[:{subset_percent}%]"

    dataset = load_dataset("cnn_dailymail", "3.0.0", split={
        'train': subset_str,
        'validation': val_str,
        'test': test_str
    })

    train_df = pd.DataFrame(dataset['train'])
    val_df = pd.DataFrame(dataset['validation'])
    test_df = pd.DataFrame(dataset['test'])

    return train_df, val_df, test_df

if __name__ == "__main__":
    train_df, val_df, test_df = load_cnn_dailymail_dataset(30)
    print("Train sample:")
    print(train_df.head())

    # Ensure the 'data' directory exists
    os.makedirs("data", exist_ok=True)

    train_df.to_csv("data/cnn_train.csv", index=False)
    val_df.to_csv("data/cnn_val.csv", index=False)
    test_df.to_csv("data/cnn_test.csv", index=False)

# preprocessing.py
import re
!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
import string
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import AutoTokenizer

# Basic text cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[(.*?)\]', '', text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text.strip()

# Preprocess text for LSTM (Keras-style tokenizer)
def preprocess_lstm(texts, num_words=10000, max_len=400):
    tokenizer = KerasTokenizer(num_words=num_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')
    return tokenizer, padded

# Preprocess for extractive models (sentence-tokenized)
def preprocess_extractive(text):
    sentences = sent_tokenize(text)
    cleaned_sentences = [clean_text(sent) for sent in sentences]
    return cleaned_sentences

# Preprocess for transformer models using HuggingFace tokenizers
def preprocess_transformer(texts, model_name="facebook/bart-base", max_len=512):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenized_inputs = tokenizer(
        texts,
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    return tokenizer, tokenized_inputs

if __name__ == "__main__":
    # Sample usage/test
    sample_texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Data science is an interdisciplinary field."
    ]
    print("Cleaned:", [clean_text(t) for t in sample_texts])
    print("LSTM:", preprocess_lstm(sample_texts)[1])
    print("Extractive:", preprocess_extractive(sample_texts[0]))
    print("Transformer:", preprocess_transformer(sample_texts)[1].input_ids.shape)

# eda_notebook.py (use this in Jupyter Notebook cell-by-cell)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
import spacy
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from textstat import flesch_reading_ease, gunning_fog

sns.set(style="darkgrid", palette="pastel")
nlp = spacy.load("en_core_web_sm")

# Load Dataset
df = pd.read_csv("data/cnn_train.csv")
df.dropna(inplace=True)
# Add Length Columns
df['article_len'] = df['article'].apply(lambda x: len(str(x).split()))
df['summary_len'] = df['highlights'].apply(lambda x: len(str(x).split()))

# 1. Dataset Overview
print("Dataset shape:", df.shape)
df.head()

# 2. Length Distributions
fig, axs = plt.subplots(1, 2, figsize=(14, 5))
sns.histplot(df['article_len'], bins=50, ax=axs[0], color='#8ECFC9')
axs[0].set_title("Article Length Distribution")
sns.histplot(df['summary_len'], bins=50, ax=axs[1], color='#FFBE7A')
axs[1].set_title("Summary Length Distribution")
plt.tight_layout()
plt.show()

#enhanced_eda_plots.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from wordcloud import WordCloud
from nltk import FreqDist, word_tokenize, download, pos_tag
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from matplotlib import cm
import spacy

# Setup
sns.set(style="whitegrid")
nlp = spacy.load("en_core_web_sm")

# Length distribution with log scale and labeled x-axis
fig, ax1 = plt.subplots(figsize=(12, 6))
article_lengths = df['article'].apply(lambda x: len(x.split()))
highlight_lengths = df['highlights'].apply(lambda x: len(x.split()))

sns.histplot(article_lengths, bins=50, kde=True, color='#66c2a5', ax=ax1, label='Articles')
sns.histplot(highlight_lengths, bins=50, kde=True, color='#fc8d62', ax=ax1, label='Highlights')
ax1.set(xscale="log")
ax1.set_title("Word Count Distribution (Log Scale)")
ax1.set_xlabel("Log of Word Count")
ax1.set_ylabel("Frequency")
ax1.set_xticks([10, 30, 100, 300, 1000, 3000])
ax1.set_xticklabels(['10', '30', '100', '300', '1k', '3k'])
plt.legend()
plt.tight_layout()
plt.show()

# Word cloud with color mask and more contrast
def plot_advanced_wordcloud(text_series, title):
    text = ' '.join(text_series.dropna())
    wordcloud = WordCloud(width=1200, height=600,
                          max_words=150,
                          background_color='black',
                          colormap='Set2').generate(text)
    plt.figure(figsize=(14, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

plot_advanced_wordcloud(df['article'], 'Advanced Word Cloud for Articles')
plot_advanced_wordcloud(df['highlights'], 'Advanced Word Cloud for Highlights')

# Most frequent words annotated and sorted
import nltk
nltk.download('stopwords') # Download stopwords if not already downloaded

stop_words = set(stopwords.words('english'))
words = word_tokenize(' '.join(df['article'].values))
words = [w.lower() for w in words if w.isalnum() and w.lower() not in stop_words]
fdist = FreqDist(words)
common = fdist.most_common(20)
words, freqs = zip(*common)

plt.figure(figsize=(12, 6))
sns.barplot(x=list(freqs), y=list(words), palette='viridis')
plt.title("Top 20 Frequent Words in Articles")
for i, v in enumerate(freqs):
    plt.text(v + 5, i, str(v), color='black', va='center')
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.show()

# POS Tag Plot with description
pos_tags = [token.pos_ for doc in nlp.pipe(df['article'][:100].astype(str)) for token in doc]
pos_df = pd.DataFrame(Counter(pos_tags).items(), columns=['POS', 'Count']).sort_values(by='Count', ascending=False)
pos_desc = {tag: spacy.explain(tag) or "" for tag in pos_df['POS']}
pos_df['Label'] = pos_df['POS'] + "\n(" + pos_df['POS'].map(pos_desc) + ")"

plt.figure(figsize=(14, 6))
sns.barplot(data=pos_df, x='Label', y='Count', palette='mako')
plt.title("POS Tag Distribution with Descriptions")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# TF-IDF with word grouping
tfidf = TfidfVectorizer(max_features=25, stop_words='english')
X = tfidf.fit_transform(df['article'][:1000])
tfidf_scores = np.asarray(X.mean(axis=0)).flatten()
words = tfidf.get_feature_names_out()

sorted_idx = np.argsort(tfidf_scores)[::-1]
words = [words[i] for i in sorted_idx]
scores = [tfidf_scores[i] for i in sorted_idx]

plt.figure(figsize=(12, 6))
sns.barplot(x=scores, y=words, palette='crest')
plt.title("Top TF-IDF Words in Articles")
plt.xlabel("Average TF-IDF Score")
plt.ylabel("Words")
plt.tight_layout()
plt.show()

from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk

# Download the VADER lexicon if not already downloaded
nltk.download('vader_lexicon')

# Sentiment Analysis Function
def plot_sentiment_analysis(text_series, title, sample_size=1000):
    sid = SentimentIntensityAnalyzer()
    text_sample = text_series[:sample_size].astype(str)
    sentiments = text_sample.apply(lambda x: sid.polarity_scores(x)['compound'])
    plt.figure(figsize=(10, 5))
    sns.histplot(sentiments, bins=50, kde=True, color='skyblue')
    plt.title(title)
    plt.xlabel('Sentiment Score')
    plt.ylabel('Frequency')
    plt.show()

plot_sentiment_analysis(df['article'], 'Sentiment Analysis of Articles')
plot_sentiment_analysis(df['highlights'], 'Sentiment Analysis of Highlights')

# Named Entity Plot Function
def plot_named_entities(text_series, title, batch_size=100):
    entity_counts = Counter()
    for i in range(0, min(len(text_series), 1000), batch_size):
        text_sample = " ".join(text_series[i : i + batch_size])
        doc = nlp(text_sample)
        entity_counts.update(ent.label_ for ent in doc.ents)

    if entity_counts:
        plt.figure(figsize=(10, 5))
        sns.barplot(x=list(entity_counts.values()), y=list(entity_counts.keys()), palette='flare')
        plt.title(title)
        plt.xlabel('Frequency')
        plt.ylabel('Entity Type')
        plt.show()
    else:
        print(f"No named entities found in the provided text for {title}.")

plot_named_entities(df['article'], 'Named Entities in Articles')
plot_named_entities(df['highlights'], 'Named Entities in Highlights')

# POS Tag Plot Function
def plot_pos_tags(text_series, title, batch_size=100):
    pos_counts = Counter()
    for i in range(0, min(len(text_series), 1000), batch_size):
        text_batch = " ".join(text_series[i : i + batch_size])
        doc = nlp(text_batch)
        pos_counts.update(token.pos_ for token in doc)

    plt.figure(figsize=(10, 5))
    sns.barplot(x=list(pos_counts.values()), y=list(pos_counts.keys()), palette='crest')
    plt.title(title)
    plt.xlabel('Frequency')
    plt.ylabel('POS Tag')
    plt.show()

plot_pos_tags(df['article'], 'POS Tags in Articles')
plot_pos_tags(df['highlights'], 'POS Tags in Highlights')

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# N-gram Frequency Plot Function
def plot_top_ngrams(text_series, ngram_range=(2, 2), top_n=20, title="Top N-grams"):
    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(text_series)
    bag_of_words = vec.transform(text_series)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    sorted_words = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]
    ngrams, counts = zip(*sorted_words)

    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(counts), y=list(ngrams), palette="Spectral")
    plt.title(title)
    plt.xlabel("Frequency")
    plt.ylabel("N-gram")
    plt.tight_layout()
    plt.show()

# Apply for bigrams and trigrams
plot_top_ngrams(df['article'].dropna().astype(str)[:1000], ngram_range=(2, 2), title="Top Bigrams in Articles")
plot_top_ngrams(df['article'].dropna().astype(str)[:1000], ngram_range=(3, 3), title="Top Trigrams in Articles")

# enhanced_eda_plots.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from wordcloud import WordCloud
from nltk import FreqDist, word_tokenize, download
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from collections import Counter
import spacy
from textstat import flesch_reading_ease
from sentence_transformers import SentenceTransformer, util
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re

# Setup
sns.set(style="whitegrid")
nlp = spacy.load("en_core_web_sm")
download('punkt')
download('stopwords')
download('vader_lexicon')

stop_words = set(stopwords.words('english'))

# Text Cleaning & Preprocessing Function
def preprocess_text(text, remove_stopwords=True, lemmatize=True):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\[.*?\]", "", text)
    text = re.sub(r"https?://\S+|www\.\S+", "", text)
    text = re.sub(r"<.*?>+", "", text)
    text = re.sub(r"[^a-zA-Z ]", "", text)
    text = text.lower()

    doc = nlp(text)
    tokens = []
    for token in doc:
        if remove_stopwords and token.text in stop_words:
            continue
        if lemmatize:
            tokens.append(token.lemma_)
        else:
            tokens.append(token.text)
    return " ".join(tokens)

# Load data and sample subset
# Updated path to include the 'data' subdirectory
full_df = pd.read_csv("data/cnn_train.csv").dropna() # Changed this line to include subdirectory
df = full_df.sample(n=2000, random_state=42).copy()

# Preprocess subset for LSTM and CNN inputs
df['clean_article'] = df['article'].apply(lambda x: preprocess_text(x))
df['clean_summary'] = df['highlights'].apply(lambda x: preprocess_text(x))

# Save for reuse
df.to_csv("cleaned_cnn_lstm_subset.csv", index=False)

# Example: Extractiveness on Clean Data
df['extractiveness'] = df.apply(lambda row: len(set(row['clean_article'].split()) & set(row['clean_summary'].split())) / max(len(row['clean_summary'].split()), 1), axis=1)
sns.histplot(df['extractiveness'], bins=30, kde=True, color="#f28500")
plt.title("Extractiveness (Word Overlap Ratio) on Cleaned Subset")
plt.xlabel("Ratio of Clean Summary Words Found in Clean Article")
plt.ylabel("Frequency")
plt.show()

# lstm_summarizer.py
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, RepeatVector
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split

# Load preprocessed data
df = pd.read_csv("cleaned_cnn_lstm_subset.csv")

# Parameters
num_words = 10000
max_article_len = 400
max_summary_len = 50
embedding_dim = 128
latent_dim = 256

# Tokenization
article_tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')
summary_tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')

article_tokenizer.fit_on_texts(df['clean_article'])
summary_tokenizer.fit_on_texts(df['clean_summary'])

article_sequences = article_tokenizer.texts_to_sequences(df['clean_article'])
summary_sequences = summary_tokenizer.texts_to_sequences(df['clean_summary'])

article_padded = pad_sequences(article_sequences, maxlen=max_article_len, padding='post', truncating='post')
summary_padded = pad_sequences(summary_sequences, maxlen=max_summary_len, padding='post', truncating='post')

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(article_padded, summary_padded, test_size=0.1, random_state=42)

# Model architecture
encoder_inputs = Input(shape=(max_article_len,))
embed_in = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_article_len)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_sequences=False)(embed_in)

repeat_vector = RepeatVector(max_summary_len)(encoder_lstm)
decoder_lstm = LSTM(latent_dim, return_sequences=True)(repeat_vector)
decoder_dense = TimeDistributed(Dense(num_words, activation='softmax'))(decoder_lstm)

model = Model(inputs=encoder_inputs, outputs=decoder_dense)
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy')
model.summary()

# Prepare decoder target
y_train = np.expand_dims(y_train, -1)
y_val = np.expand_dims(y_val, -1)

# Training
model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Save model
model.save("lstm_summarizer_model.h5")

# pegasus_summarizer_inference.py
import pandas as pd
from transformers import PegasusTokenizer, PegasusForConditionalGeneration

# Load cleaned test data (subset)
df = pd.read_csv("cleaned_cnn_lstm_subset.csv").sample(100, random_state=42)
articles = df['clean_article'].tolist()

# Load pretrained PEGASUS model
tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-cnn_dailymail")

# Summarization function
def summarize(text):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, max_length=512)
    summary_ids = model.generate(inputs["input_ids"], max_length=64, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Generate summaries
summaries = [summarize(article) for article in articles]

# Save results
df_result = df.copy()
df_result["pegasus_summary"] = summaries
df_result.to_csv("pegasus_pretrained_inference.csv", index=False)
print("âœ… Inference complete. Summaries saved to 'pegasus_pretrained_inference.csv'")

# extractive_models.py
import re
import string
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.tokenize import sent_tokenize

# Inline clean_text function to avoid import issues
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[(.*?)\]', '', text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text.strip()

# TextRank Algorithm for Extractive Summarization
def textrank_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)

    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
    np.fill_diagonal(similarity_matrix, 0)

    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)

    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    summary = " ".join([s for _, s in ranked_sentences[:num_sentences]])
    return summary

# LSA-based Extractive Summarization
def lsa_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)

    svd = TruncatedSVD(n_components=1)
    lsa_scores = svd.fit_transform(tfidf_matrix)

    ranked = sorted(((score, sent) for score, sent in zip(lsa_scores, sentences)), reverse=True)
    summary = " ".join([s for _, s in ranked[:num_sentences]])
    return summary

if __name__ == "__main__":
    sample = """
    The quick brown fox jumps over the lazy dog.
    This sentence is often used to test typewriters and fonts.
    It contains every letter in the English alphabet.
    It's short and well known.
    """
    print("TextRank Summary:")
    print(textrank_summarize(sample))

    print("\nLSA Summary:")
    print(lsa_summarize(sample))

import re
import string
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.tokenize import sent_tokenize

# Clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[(.*?)\]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text.strip()

# TextRank
def textrank_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
    np.fill_diagonal(similarity_matrix, 0)
    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)
    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

# LSA
def lsa_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    svd = TruncatedSVD(n_components=1)
    lsa_scores = svd.fit_transform(tfidf_matrix)
    ranked = sorted(((score, sent) for score, sent in zip(lsa_scores, sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer

# Load your cleaned dataset
df = pd.read_csv("cleaned_cnn_lstm_subset.csv")

# Initialize tokenizers
article_tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
summary_tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')

# Fit tokenizers
article_tokenizer.fit_on_texts(df['clean_article'])
summary_tokenizer.fit_on_texts(df['clean_summary'])

# Save both tokenizers
import pickle
with open("lstm_tokenizers.pkl", "wb") as f:
    pickle.dump((article_tokenizer, summary_tokenizer), f)

print("âœ… Tokenizers recreated and saved as 'lstm_tokenizers.pkl'")

# compare_summaries.py
import pandas as pd
from rouge_score import rouge_scorer
# Instead of importing the module, import the functions directly
#from extractive_models import textrank_summarize, lsa_summarize
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pickle
from transformers import PegasusTokenizer, PegasusForConditionalGeneration

# ... (rest of the code remains the same) ...

# Load test data
print("Loading data...")
df = pd.read_csv("cleaned_cnn_lstm_subset.csv").sample(100, random_state=42)
articles = df['clean_article'].tolist()
references = df['clean_summary'].tolist()

# PEGASUS inference
print("Loading PEGASUS...")
pegasus_tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
pegasus_model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-cnn_dailymail")

def summarize_pegasus(text):
    inputs = pegasus_tokenizer([text], return_tensors="pt", truncation=True, max_length=512)
    summary_ids = pegasus_model.generate(inputs["input_ids"], max_length=64, num_beams=4, early_stopping=True)
    return pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# LSTM inference
print("Loading LSTM model...")
lstm_model = load_model("lstm_summarizer_model.h5")
with open("lstm_tokenizers.pkl", "rb") as f:
    article_tokenizer, summary_tokenizer = pickle.load(f)

def summarize_lstm(text):
    seq = article_tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=400, padding='post')
    pred = lstm_model.predict(padded)
    pred_ids = np.argmax(pred[0], axis=1)
    words = [k for k, v in summary_tokenizer.word_index.items() if v in pred_ids and v < 10000]
    return " ".join(words)

# ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

print("Generating summaries and scoring...")
results = []

for i in range(len(articles)):
    row = {
        "reference": references[i],
        "cnn_textrank": textrank_summarize(articles[i]),
        "cnn_lsa": lsa_summarize(articles[i]),
        "lstm": summarize_lstm(articles[i]),
        "pegasus": summarize_pegasus(articles[i])
    }

    for model in ["cnn_textrank", "cnn_lsa", "lstm", "pegasus"]:
        scores = scorer.score(row["reference"], row[model])
        row[f"{model}_rouge1"] = round(scores["rouge1"].fmeasure, 4)
        row[f"{model}_rouge2"] = round(scores["rouge2"].fmeasure, 4)
        row[f"{model}_rougeL"] = round(scores["rougeL"].fmeasure, 4)

    results.append(row)

# Save results
df_results = pd.DataFrame(results)
df_results.to_csv("model_comparison_summary_scores.csv", index=False)
print("âœ… Model comparison complete. Results saved to 'model_comparison_summary_scores.csv'")

import pandas as pd

# Load comparison results
df = pd.read_csv("model_comparison_summary_scores.csv")

# Define models and ROUGE metrics
model_names = ["cnn_textrank", "cnn_lsa", "lstm", "pegasus"]
rouge_metrics = ["rouge1", "rouge2", "rougeL"]

# Compute average ROUGE scores
avg_scores = {}
for model in model_names:
    avg_scores[model] = {
        metric: round(df[f"{model}_{metric}"].mean(), 4) for metric in rouge_metrics
    }

comparison_df = pd.DataFrame(avg_scores).T
comparison_df.columns = [f"ROUGE-{col[-1]}" for col in comparison_df.columns]
print("ðŸ“Š Average ROUGE Score Comparison Table")
display(comparison_df)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
comparison_df.plot(kind="bar", figsize=(10, 6), colormap="Set2", edgecolor='black')
plt.title("Average ROUGE Scores by Model", fontsize=14)
plt.ylabel("Score", fontsize=12)
plt.xticks(rotation=0)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

ranked_models = comparison_df.sort_values("ROUGE-L", ascending=False)
print("ðŸ† Models Ranked by ROUGE-L:")
display(ranked_models)

# streamlit_summary_app.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pickle
import re
import string
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.tokenize import sent_tokenize

# Load models and tokenizers
@st.cache_resource
def load_models():
    lstm_model = load_model("lstm_summarizer_model.h5")
    with open("lstm_tokenizers.pkl", "rb") as f:
        article_tokenizer, summary_tokenizer = pickle.load(f)
    pegasus_tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
    pegasus_model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-cnn_dailymail")
    return lstm_model, article_tokenizer, summary_tokenizer, pegasus_tokenizer, pegasus_model

# Generate summaries
def summarize_lstm(text, model, tokenizer):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=400, padding='post')
    pred = model.predict(padded)
    pred_ids = np.argmax(pred[0], axis=1)
    reverse_index = {v: k for k, v in tokenizer.word_index.items()}
    words = [reverse_index.get(i, '') for i in pred_ids if i in reverse_index and i < 10000]
    return " ".join(words)

def summarize_pegasus(text, tokenizer, model):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, max_length=512)
    summary_ids = model.generate(inputs["input_ids"], max_length=64, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[(.*?)\]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text.strip()

# TextRank
def textrank_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
    np.fill_diagonal(similarity_matrix, 0)
    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)
    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

# LSA
def lsa_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    svd = TruncatedSVD(n_components=1)
    lsa_scores = svd.fit_transform(tfidf_matrix)
    ranked = sorted(((score, sent) for score, sent in zip(lsa_scores, sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

# UI
st.title("ðŸ“„ Text Summarization Model Comparison")

# Input
input_text = st.text_area("Enter an article to summarize:", height=300)

if st.button("Summarize") and input_text:
    st.info("Generating summaries. Please wait...")
    lstm_model, article_tokenizer, summary_tokenizer, pegasus_tokenizer, pegasus_model = load_models()

    # Run models
    summary_textrank = textrank_summarize(input_text)
    summary_lsa = lsa_summarize(input_text)
    summary_lstm = summarize_lstm(input_text, lstm_model, article_tokenizer)
    summary_pegasus = summarize_pegasus(input_text, pegasus_tokenizer, pegasus_model)

    # Display
    st.subheader("ðŸŸ¦ CNN - TextRank")
    st.write(summary_textrank)

    st.subheader("ðŸŸ¨ CNN - LSA")
    st.write(summary_lsa)

    st.subheader("ðŸŸ§ LSTM")
    st.write(summary_lstm)

    st.subheader("ðŸŸ© Transformer - PEGASUS")
    st.write(summary_pegasus)

# Optional: Load and show comparison table
st.sidebar.title("ðŸ“Š Accuracy Comparison")
if st.sidebar.checkbox("Show Model ROUGE Scores"):
    df = pd.read_csv("model_comparison_summary_scores.csv")
    model_names = ["cnn_textrank", "cnn_lsa", "lstm", "pegasus"]
    rouge_metrics = ["rouge1", "rouge2", "rougeL"]
    avg_scores = {model: {metric: round(df[f"{model}_{metric}"].mean(), 4) for metric in rouge_metrics} for model in model_names}
    comp_df = pd.DataFrame(avg_scores).T
    comp_df.columns = [f"ROUGE-{m[-1]}" for m in comp_df.columns]

    st.sidebar.dataframe(comp_df)

    st.sidebar.markdown("### ðŸ“ˆ Bar Chart")
    st.sidebar.pyplot(
        comp_df.plot(kind="bar", figsize=(8, 4), colormap="Set2", edgecolor='black').figure
    )

!pip install streamlit pyngrok --quiet

from tensorflow.keras.models import load_model  # ðŸ‘ˆ Add this
code = '''
# streamlit_summary_app.py
from keras.models import load_model
from tensorflow.keras.models import load_model  # ðŸ‘ˆ Add this
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pickle
import re
import string
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.tokenize import sent_tokenize

# Load models and tokenizers
@st.cache_resource
def load_models():
    lstm_model = load_model("lstm_summarizer_model.h5")
    with open("lstm_tokenizers.pkl", "rb") as f:
        article_tokenizer, summary_tokenizer = pickle.load(f)
    pegasus_tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
    pegasus_model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-cnn_dailymail")
    return lstm_model, article_tokenizer, summary_tokenizer, pegasus_tokenizer, pegasus_model

# Generate summaries
def summarize_lstm(text, model, tokenizer):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=400, padding='post')
    pred = model.predict(padded)
    pred_ids = np.argmax(pred[0], axis=1)
    reverse_index = {v: k for k, v in tokenizer.word_index.items()}
    words = [reverse_index.get(i, '') for i in pred_ids if i in reverse_index and i < 10000]
    return " ".join(words)

def summarize_pegasus(text, tokenizer, model):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, max_length=512)
    summary_ids = model.generate(inputs["input_ids"], max_length=64, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[(.*?)\]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text.strip()

# TextRank
def textrank_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
    np.fill_diagonal(similarity_matrix, 0)
    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)
    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

# LSA
def lsa_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    cleaned = [clean_text(sent) for sent in sentences]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned)
    svd = TruncatedSVD(n_components=1)
    lsa_scores = svd.fit_transform(tfidf_matrix)
    ranked = sorted(((score, sent) for score, sent in zip(lsa_scores, sentences)), reverse=True)
    return ' '.join([s for _, s in ranked[:num_sentences]])

# UI
st.title("ðŸ“„ Text Summarization Model Comparison")

# Input
input_text = st.text_area("Enter an article to summarize:", height=300)

if st.button("Summarize") and input_text:
    st.info("Generating summaries. Please wait...")
    lstm_model, article_tokenizer, summary_tokenizer, pegasus_tokenizer, pegasus_model = load_models()

    # Run models
    summary_textrank = textrank_summarize(input_text)
    summary_lsa = lsa_summarize(input_text)
    summary_lstm = summarize_lstm(input_text, lstm_model, article_tokenizer)
    summary_pegasus = summarize_pegasus(input_text, pegasus_tokenizer, pegasus_model)

    # Display
    st.subheader("ðŸŸ¦ CNN - TextRank")
    st.write(summary_textrank)

    st.subheader("ðŸŸ¨ CNN - LSA")
    st.write(summary_lsa)

    st.subheader("ðŸŸ§ LSTM")
    st.write(summary_lstm)

    st.subheader("ðŸŸ© Transformer - PEGASUS")
    st.write(summary_pegasus)

# Optional: Load and show comparison table
st.sidebar.title("ðŸ“Š Accuracy Comparison")
if st.sidebar.checkbox("Show Model ROUGE Scores"):
    df = pd.read_csv("model_comparison_summary_scores.csv")
    model_names = ["cnn_textrank", "cnn_lsa", "lstm", "pegasus"]
    rouge_metrics = ["rouge1", "rouge2", "rougeL"]
    avg_scores = {model: {metric: round(df[f"{model}_{metric}"].mean(), 4) for metric in rouge_metrics} for model in model_names}
    comp_df = pd.DataFrame(avg_scores).T
    comp_df.columns = [f"ROUGE-{m[-1]}" for m in comp_df.columns]

    st.sidebar.dataframe(comp_df)

    st.sidebar.markdown("### ðŸ“ˆ Bar Chart")
    st.sidebar.pyplot(
        comp_df.plot(kind="bar", figsize=(8, 4), colormap="Set2", edgecolor='black').figure
    )
'''
with open("streamlit_summary_app.py", "w") as f:
    f.write(code)
# Step 1: Install (if not already)
!pip install streamlit pyngrok --quiet

# Step 2: Set Authtoken
from pyngrok import ngrok

# Replace with your actual ngrok authtoken
ngrok.set_auth_token("2v3aYHTlQINzjtZdVc9CQDM3Bce_2K99UE7cpvdPLadLdK22K")

from pyngrok import ngrok
import os

# Kill any existing tunnel
ngrok.kill()

# ðŸ” Restart Streamlit
!fuser -k 8501/tcp || true  # kill any streamlit stuck
!streamlit run streamlit_summary_app.py &>/dev/null &

# ðŸ”— Create a new tunnel to localhost
public_url = ngrok.connect("http://localhost:8501")
print(f"âœ… App should now be available at: {public_url}")

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, Concatenate, TimeDistributed

# Hyperparameters
vocab_size = 10000
embedding_dim = 128
latent_dim = 256
max_text_len = 400
max_summary_len = 50

# Encoder
encoder_inputs = Input(shape=(max_text_len,))
enc_emb = Embedding(vocab_size, embedding_dim, trainable=True)(encoder_inputs)
encoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4))
encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_emb)
state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

# Decoder
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_size, embedding_dim, trainable=True)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])

# Attention
attention = tf.keras.layers.Attention()
context_vector = attention([decoder_outputs, encoder_outputs])
decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])

# Output layer
output_layer = TimeDistributed(Dense(vocab_size, activation='softmax'))
decoder_outputs = output_layer(decoder_combined_context)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize tokenizer
num_words = 10000
text_tokenizer = Tokenizer(num_words=num_words, oov_token="<OOV>")
text_tokenizer.fit_on_texts(train_df['article'])

# Tokenize and pad encoder input
encoder_input_data = text_tokenizer.texts_to_sequences(train_df['article'])
encoder_input_data = pad_sequences(encoder_input_data, maxlen=400, padding='post', truncating='post')

summary_tokenizer = Tokenizer(num_words=num_words, oov_token="<OOV>")
summary_tokenizer.fit_on_texts(train_df['highlights'])

# Convert summaries to sequences
summary_seq = summary_tokenizer.texts_to_sequences(train_df['highlights'])
summary_seq = pad_sequences(summary_seq, maxlen=50, padding='post', truncating='post')

# Decoder input is the same, but shifted one position right
decoder_input_data = summary_seq[:, :-1]
decoder_target_data = summary_seq[:, 1:]

# Expand dimensions of target for sparse_categorical_crossentropy
decoder_target_data = decoder_target_data[..., np.newaxis]

history = model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=20,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    ]
)

